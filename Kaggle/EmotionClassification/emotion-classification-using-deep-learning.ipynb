{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('/kaggle/input/emotion-classification/emotion.data')\n",
    "data_df.columns = ['id', 'text', 'emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file):\n",
    "    \"\"\"\n",
    "    input: embeddings file\n",
    "    output: embedding index\n",
    "    \"\"\"\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_folder = \"/kaggle/input/glove6b50dtxt/\"\n",
    "emb_file_name = 'glove.6B.50d.txt'\n",
    "emb_path = os.path.join(emb_folder, emb_file_name)\n",
    "emb_glove = load_embeddings(emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  emotion\n",
       "0   27383  i feel awful about it too because it s my job ...  sadness\n",
       "1  110083                              im alone i feel awful  sadness\n",
       "2  140764  ive probably mentioned this before but i reall...      joy\n",
       "3  100071           i was feeling a little low few days back  sadness\n",
       "4    2837  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    \"\"\"\n",
    "    input: list of list of words\n",
    "    output: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    print('build vocabulary')\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "import operator\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    '''\n",
    "    input: vocabulary, embedding index\n",
    "    output: list of unknown words; also prints the vocabulary coverage of embeddings and the % of comments text covered by the embeddings\n",
    "    '''\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in tqdm(vocab.keys()):\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "    print('Found embeddings for {:.3%} of vocabulary'.format(len(known_words)/len(vocab)))\n",
    "    print('Found embeddings for {:.3%} of all text'.format(nb_known_words/(nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return unknown_words                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416809/416809 [00:02<00:00, 169725.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75302\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocabulary(data_df['text'])\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75302/75302 [00:00<00:00, 573383.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 73.889% of vocabulary\n",
      "Found embeddings for 99.549% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oof_emb = check_coverage(vocabulary, emb_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('href', 4788),\n",
       " ('shouldnt', 777),\n",
       " ('hadnt', 394),\n",
       " ('youve', 337),\n",
       " ('werent', 269),\n",
       " ('itll', 220),\n",
       " ('nofollow', 202),\n",
       " ('hadn', 198),\n",
       " ('theyve', 183),\n",
       " ('everyones', 158),\n",
       " ('theyll', 152),\n",
       " ('pagetitle', 122),\n",
       " ('hahaha', 114),\n",
       " ('wouldve', 95),\n",
       " ('permalink', 92),\n",
       " ('theyd', 87),\n",
       " ('idk', 72),\n",
       " ('anyones', 70),\n",
       " ('everythings', 67),\n",
       " ('feedlinks', 62)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_emb[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    '''\n",
    "    input: embedding matrix\n",
    "    output: known contractions (from embeddings)\n",
    "    '''\n",
    "    known = []\n",
    "    for contract in tqdm_notebook(contraction_mapping):\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "def clean_contractions(text, mapping=contraction_mapping):\n",
    "    '''\n",
    "    input: current text, contraction mappings\n",
    "    output: modify the comments to use the base form from contraction mapping\n",
    "    '''\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['text'] = data_df['text'].apply(lambda x: clean_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416809/416809 [00:02<00:00, 177681.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75291\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocabulary(data_df['text'])\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75291/75291 [00:00<00:00, 561259.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 73.888% of vocabulary\n",
      "Found embeddings for 99.549% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oof_emb = check_coverage(vocabulary, emb_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                      \"href\": \" \", \"shouldnt\": \"should not\", \"hadnt\": \"had not\", \"werent\": \"were not\", \"itll\": \"it will\",\n",
    "                      \"nofollow\": \"no follow\", \"theyve\":\"they have\", \"everyones\": \"everyone\", \"theyll\": \"they will\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416809/416809 [00:02<00:00, 176357.01it/s]\n",
      "100%|██████████| 75282/75282 [00:00<00:00, 590028.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75282\n",
      "Found embeddings for 73.897% of vocabulary\n",
      "Found embeddings for 99.638% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#apply new entried contractions clean\n",
    "data_df['text'] = data_df['text'].apply(lambda x: clean_contractions(x,new_contraction_mapping))\n",
    "\n",
    "# build the vocabulary\n",
    "vocabulary = build_vocabulary(data_df['text'])\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "\n",
    "# check the vocabulary coverage\n",
    "oof_emb = check_coverage(vocabulary, emb_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not covered vocabulary: 19651; first 20 most frequent: [('youve', 337), ('hadn', 198), ('pagetitle', 122), ('hahaha', 114), ('wouldve', 95), ('permalink', 92), ('theyd', 87), ('idk', 72), ('anyones', 70), ('everythings', 67), ('feedlinks', 62), ('languagedirection', 62), ('isprivate', 62), ('utf', 59), ('couldve', 57), ('pissy', 55), ('mobileclass', 51), ('ismobile', 51), ('shouldve', 47), ('onclick', 45)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Not covered vocabulary: {len(oof_emb)}; first 20 most frequent: {list(oof_emb[0:20])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Input, LSTM, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, SpatialDropout1D, Embedding, add\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "EMBED_SIZE = 50  # size of word vector; this should be set to 50 to match the embedding source\n",
    "MAX_FEATURES = 200000  # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAXLEN = 50 # max length of  text\n",
    "\n",
    "NO_EPOCHS = 30  # training epochs\n",
    "VERBOSE = 1  # report frequency while training\n",
    "PATIENCE = 5  # number of training epochs for validation not improved before stoping training\n",
    "BATCH_SIZE = 128  # size of training batches\n",
    "LSTM_UNITS = 128  # size of LSTM units\n",
    "DENSE_HIDDEN_UNITS = 256  # size of dense hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(f\"Found {len(word_index)} unique tokens.\")\n",
    "    return word_index, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(word_index, emb_glove):\n",
    "    print('build embedding matrix')\n",
    "    embeddings_index = emb_glove\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(data_df, tokenizer):\n",
    "    print(\"build model\")\n",
    "\n",
    "    print(\"label encoding\")\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(data_df['text_proc'])\n",
    "    data = pad_sequences(sequences, maxlen=MAXLEN)\n",
    "    labels = data_df.emotion.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    X = data\n",
    "    y = labels\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print('Shape of data tensor:', X.shape)\n",
    "    print('Shape of label tensor:', y.shape)\n",
    "    print(f'Data train/valid: {x_train.shape}/{x_val.shape} Label train/valid:  {y_train.shape}/{y_val.shape}')\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model(word_index, embedding_matrix, x_train, x_val, y_train, y_val):\n",
    "    DENSE_OUTPUT = y_val.shape[1]\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=PATIENCE, verbose=VERBOSE)\n",
    "    checkpointer = ModelCheckpoint('best_model.b5',\n",
    "                                    monitor='val_acc',\n",
    "                                    verbose=VERBOSE,\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=True)\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBED_SIZE,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAXLEN,\n",
    "                                trainable=False)\n",
    "\n",
    "    model = None    \n",
    "    sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = SpatialDropout1D(0.4)(embedded_sequences)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    hidden = GlobalMaxPooling1D()(x)\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(DENSE_OUTPUT, activation='sigmoid')(hidden)\n",
    "\n",
    "    model = Model(sequence_input, result)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    print(\"run model - run and save the model pipeline\")\n",
    "    train_model  = model.fit(x_train, y_train, validation_data=[x_val, y_val],\n",
    "                        epochs=NO_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        callbacks=[earlystopper, checkpointer],verbose=VERBOSE)\n",
    "    print(\"run model - predict validation set\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_model(model, x_val, y_val):\n",
    "    print(\"run model - predict validation set\")\n",
    "    score = model.evaluate(x_val, y_val, verbose=0)\n",
    "    print(f'Last validation loss: {score[0]}, accuracy: {score[1]}')\n",
    "    # load saved optimal model\n",
    "    model_optimal = model\n",
    "    model_optimal.load_weights('best_model.b5')\n",
    "    score = model_optimal.evaluate(x_val, y_val, verbose=0)\n",
    "    print(f'Best validation loss: {score[0]}, accuracy: {score[1]}')\n",
    "\n",
    "    print(\"run model - check prediction accuracy | precision | recall | F1-score\")\n",
    "    y_pred = model_optimal.predict(x_val)\n",
    "    true_val = np.argmax(y_val, axis=1)\n",
    "    pred_val = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(true_val, pred_val))\n",
    "    print(\"run model - completed validation\")\n",
    "    pickle.dump(model_optimal,open(os.path.join('.', \"emotion.model\"),'wb'))\n",
    "    print(\"best model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run_model(data_df, embeddings):\n",
    "    word_index, tokenizer = tokenize(data_df['text_proc'])\n",
    "    embedding_matrix = build_embedding_matrix(word_index, embeddings)\n",
    "    x_train, x_val, y_train, y_val = prepare_model(data_df, tokenizer)\n",
    "    model = build_train_model(word_index, embedding_matrix, x_train, x_val, y_train, y_val)\n",
    "    validation_model(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE = 50000\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#train = data_df.sample(SAMPLE)\n",
    "train = data_df\n",
    "train['text_proc'] = train['text']\n",
    "train_y = train.emotion.values\n",
    "le = LabelEncoder()\n",
    "le.fit(train_y)\n",
    "train.emotion = le.transform(train_y)\n",
    "le_name_mapping = dict(zip(le.transform(le.classes_), le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 34992/75282 [00:00<00:00, 349911.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75282 unique tokens.\n",
      "build embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75282/75282 [00:00<00:00, 368201.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n",
      "label encoding\n",
      "Shape of data tensor: (416809, 50)\n",
      "Shape of label tensor: (416809, 6)\n",
      "Data train/valid: (333447, 50)/(83362, 50) Label train/valid:  (333447, 6)/(83362, 6)\n",
      "run model - run and save the model pipeline\n",
      "Train on 333447 samples, validate on 83362 samples\n",
      "Epoch 1/30\n",
      "333447/333447 [==============================] - 257s 772us/step - loss: 0.8855 - acc: 0.6615 - val_loss: 0.7308 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69411, saving model to best_model.b5\n",
      "Epoch 2/30\n",
      "333447/333447 [==============================] - 259s 776us/step - loss: 0.5161 - acc: 0.8027 - val_loss: 0.3600 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69411 to 0.85176, saving model to best_model.b5\n",
      "Epoch 3/30\n",
      "333447/333447 [==============================] - 254s 762us/step - loss: 0.4024 - acc: 0.8426 - val_loss: 0.1714 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85176 to 0.91699, saving model to best_model.b5\n",
      "Epoch 4/30\n",
      "333447/333447 [==============================] - 254s 762us/step - loss: 0.3447 - acc: 0.8609 - val_loss: 0.1657 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91699 to 0.91876, saving model to best_model.b5\n",
      "Epoch 5/30\n",
      "272128/333447 [=======================>......] - ETA: 40s - loss: 1.1921e-07 - acc: 0.1380"
     ]
    }
   ],
   "source": [
    "word_index, tokenizer = tokenize(data_df['text_proc'])\n",
    "embedding_matrix = build_embedding_matrix(word_index, emb_glove)\n",
    "x_train, x_val, y_train, y_val = prepare_model(data_df, tokenizer)\n",
    "model = build_train_model(word_index, embedding_matrix, x_train, x_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run model - predict validation set\n",
      "Last validation loss: 1.1920930376163597e-07, accuracy: 0.1360212117433548\n",
      "Best validation loss: 0.11276268255414988, accuracy: 0.9342146515846252\n",
      "run model - check prediction accuracy | precision | recall | F1-score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     11339\n",
      "           1       0.93      0.86      0.89      9376\n",
      "           2       0.98      0.92      0.95     28247\n",
      "           3       0.79      0.93      0.86      6853\n",
      "           4       0.98      0.96      0.97     24504\n",
      "           5       0.75      0.96      0.84      3043\n",
      "\n",
      "    accuracy                           0.93     83362\n",
      "   macro avg       0.89      0.93      0.91     83362\n",
      "weighted avg       0.94      0.93      0.94     83362\n",
      "\n",
      "run model - completed validation\n",
      "best model saved\n"
     ]
    }
   ],
   "source": [
    "validation_model(model, x_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
