{
  "cells": [
    {
      "metadata": {
        "_uuid": "805a97911f01abb36ff8414d8d6aec9293e4fb85"
      },
      "cell_type": "markdown",
      "source": "* <h1><center><font size=\"6\">CNN with Keras for Fashion MNIST</font></center></h1>\n\n\n<img src=\"https://kaggle2.blob.core.windows.net/datasets-images/2243/3791/9384af51de8baa77f6320901f53bd26b/dataset-card.png\" width=\"400\"></img>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Load packages</a>  \n- <a href='#3'>Read the data</a>  \n- <a href='#4'>Data exploration</a>\n    - <a href='#41'>Class distribution</a>\n    - <a href='#42'>Images samples</a>\n- <a href='#5'>Model</a>  \n    - <a href='#51'>Prepare the model</a>  \n    - <a href='#52'>Train the model</a>  \n- <a href='#6'>Prediction accuracy</a>   \n    - <a href='#61'>Evaluate the model score</a>   \n    - <a href='#62'>Correctly classified images</a>   \n    - <a href='#63'>Incorrectly classified images</a>   \n- <a href='#7'>Conclusions</a>\n- <a href='#8'>References</a>"
    },
    {
      "metadata": {
        "_uuid": "1b1c16628c2f62a18e1dc2068e1d67d7003922b1"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"1\">Introduction</a>  \n\n\n## Dataset\n\nFashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\n\n## Content\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.   \n\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.   \n\nThe training and test data sets have 785 columns.   \n\nThe first column consists of the class labels (see above), and represents the article of clothing. \n\nThe rest of 784 columns (1-785) contain the pixel-values of the associated image."
    },
    {
      "metadata": {
        "_uuid": "5a708dc52b2e5990e2247ed573d50d0f6933b730"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"2\">Load packages</a>"
    },
    {
      "metadata": {
        "_uuid": "2defa674e4e6d0e7371df92e7d1f388fc5c14bb6",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\nimport matplotlib.pyplot as plt\n%matplotlib inline ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e17fd2539412442ddb3ecacf84366ddd62faf836"
      },
      "cell_type": "markdown",
      "source": "## Parameters"
    },
    {
      "metadata": {
        "_uuid": "d7a44bfc7a28df7026241c4a7e047298446f1554",
        "trusted": true
      },
      "cell_type": "code",
      "source": "IMG_ROWS = 28\nIMG_COLS = 28\nNUM_CLASSES = 10\nTEST_SIZE = 0.2\nRANDOM_STATE = 2018\n#Model\nNO_EPOCHS = 50\nBATCH_SIZE = 128\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"../input/fashionmnist/\"\nelse:\n    PATH=\"../input/\"\nprint(os.listdir(PATH))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f49d65b4af77e87ab34b6854850619911cff1e6d"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"3\">Read the data</a>\n\nThere are 10 different classes of images, as following: \n\n* **0**: **T-shirt/top**;   \n* **1**: **Trouser**;   \n* **2**: **Pullover**;   \n* **3**: **Dress**;\n* **4**: **Coat**;\n* **5**: **Sandal**;\n* **6**: **Shirt**;\n* **7**: **Sneaker**;\n* **8**: **Bag**;\n* **9**: **Ankle boot**.\n\nImage dimmensions are **28**x**28**.   \n\nThe train set and test set are given in two separate datasets.\n"
    },
    {
      "metadata": {
        "_uuid": "a9c3148fda056ecc88570b302f5185064d5e9fc8",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_file = PATH+\"fashion-mnist_train.csv\"\ntest_file  = PATH+\"fashion-mnist_test.csv\"\n\ntrain_data = pd.read_csv(train_file)\ntest_data = pd.read_csv(test_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "290f8f38b2f64dfd1bd3432eb1e9a101dbbaf4e5"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"4\">Data exploration</a>"
    },
    {
      "metadata": {
        "_uuid": "e8891ae7996aa2ae5bc1c025661a77ae91c2fdfb"
      },
      "cell_type": "markdown",
      "source": "![](http://)The dimmension of the original  train,  test set are as following:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a133496cefc53baa56b9a5eec93bfd064ea6360d"
      },
      "cell_type": "code",
      "source": "print(\"Fashion MNIST train -  rows:\",train_data.shape[0],\" columns:\", train_data.shape[1])\nprint(\"Fashion MNIST test -  rows:\",test_data.shape[0],\" columns:\", test_data.shape[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc317fbf40c9b2838f8091e745a64e060a7affc0"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"41\">Class distribution</a>\n\nLet's see how many number of images are in each class. We start with the train set.\n\n### Train set images class distribution"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da948eb587daf81160a4794a36662f8872c882a4"
      },
      "cell_type": "code",
      "source": "# Create a dictionary for each type of label \nlabels = {0 : \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n\ndef get_classes_distribution(data):\n    # Get the count for each label\n    label_counts = data[\"label\"].value_counts()\n\n    # Get total number of samples\n    total_samples = len(data)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count / total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n\nget_classes_distribution(train_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc41d889d3ad6352994cdf8a76a78ed0d22ec141"
      },
      "cell_type": "markdown",
      "source": "The classes are equaly distributed in the train set (10% each). Let's check the same for the test set.\n\n### Test set images class distribution"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2581093ff1268ae9487fe83b383c2d4be4657000"
      },
      "cell_type": "code",
      "source": "get_classes_distribution(test_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fffa6f5248eb037b47a84e1e1ee889ee3470a1d4"
      },
      "cell_type": "markdown",
      "source": "Also in the test set the 10 classes are equaly distributed (10% each)."
    },
    {
      "metadata": {
        "_uuid": "588beb3c38b5f5a0b41ca0699426a1477efb8412"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"42\">Sample images</a>\n\n### Train set images\n\nLet's plot some samples for the images."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62fd41c12740f184ec160d692b3289d4457297d6"
      },
      "cell_type": "code",
      "source": "def sample_images_data(data):\n    # An empty list to collect some samples\n    sample_images = []\n    sample_labels = []\n\n    # Iterate over the keys of the labels dictionary defined in the above cell\n    for k in labels.keys():\n        # Get four samples for each category\n        samples = data[data[\"label\"] == k].head(4)\n        # Append the samples to the samples list\n        for j, s in enumerate(samples.values):\n            # First column contain labels, hence index should start from 1\n            img = np.array(samples.iloc[j, 1:]).reshape(IMG_ROWS,IMG_COLS)\n            sample_images.append(img)\n            sample_labels.append(samples.iloc[j, 0])\n\n    print(\"Total number of sample images to plot: \", len(sample_images))\n    return sample_images, sample_labels\n\ntrain_sample_images, train_sample_labels = sample_images_data(train_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5c80cef30985bb5ab0ee48d4fc9891660c2a92f"
      },
      "cell_type": "markdown",
      "source": "Let's now plot the images."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f4f27f5143bdd834302a2c43cdb1d68d2a131a9"
      },
      "cell_type": "code",
      "source": "def plot_sample_images(data_sample_images,data_sample_labels,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,8, figsize=(16,10))\n\n    for i, img in enumerate(data_sample_images):\n        ax[i//8, i%8].imshow(img, cmap=cmap)\n        ax[i//8, i%8].axis('off')\n        ax[i//8, i%8].set_title(labels[data_sample_labels[i]])\n    plt.show()    \n    \nplot_sample_images(train_sample_images,train_sample_labels, \"Greens\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd30352b2674a20eca5e792d7c186cdcbab25461"
      },
      "cell_type": "markdown",
      "source": "### Test set images\n\nLet's plot now a selection of the train set images."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "616df96bfae0e8065c206e6dfec2847d5d42125d"
      },
      "cell_type": "code",
      "source": "test_sample_images, test_sample_labels = sample_images_data(test_data)\nplot_sample_images(test_sample_images,test_sample_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "273c79c7470c49fe7a546dd7148e0536be2dee19"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"5\">Model</a>\n\nWe start with preparing the model."
    },
    {
      "metadata": {
        "_uuid": "10b920b2ffc5de4a3ddebe494966d90cca91ee95"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"51\">Prepare the model</a>\n\n## Data preprocessing\n\nFirst we will do a data preprocessing to prepare for the model.\n\nWe reshape the columns  from (784) to (28,28,1). We also save label (target) feature as a separate vector."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "db4c308b9fb334a54b4c055b00d2a1fbcf77ab96"
      },
      "cell_type": "code",
      "source": "# data preprocessing\ndef data_preprocessing(raw):\n    out_y = keras.utils.to_categorical(raw.label, NUM_CLASSES)\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n    out_x = x_shaped_array / 255\n    return out_x, out_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d3cafa55173d40cd9a42df63d4919f03b264c09"
      },
      "cell_type": "markdown",
      "source": "We process both the train_data and the test_data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "454d7a8fdca4bdbefc04a9d796de04b6af4a1767"
      },
      "cell_type": "code",
      "source": "# prepare the data\nX, y = data_preprocessing(train_data)\nX_test, y_test = data_preprocessing(test_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e25c569f0a0e817ca0462f3e3ea2f50feb480b0"
      },
      "cell_type": "markdown",
      "source": "## Split train in train and validation set\n\nWe further split the train set in train and validation set. The validation set will be 20% from the original train set, therefore the split will be train/validation of 0.8/0.2."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "36eb910dd0868a227a73c9759d6aee0a47ba2b1e"
      },
      "cell_type": "code",
      "source": "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e623ca7c0c61634c0e4b890fc5cf5cacb132552"
      },
      "cell_type": "markdown",
      "source": "The dimmension of the processed train, validation and test set are as following:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98b3f1bf07e8ed614bf32b8e1955cbe90bff21c5"
      },
      "cell_type": "code",
      "source": "print(\"Fashion MNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\nprint(\"Fashion MNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\nprint(\"Fashion MNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd9405b5a3fe0c7bd5bebe0616190a8cf26d2811"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"52\">Train the model</a>\n\n### Build the model   \n\n\n\nWe will use a **Sequential** model.\n* The **Sequential** model is a linear stack of layers. It can be first initialized and then we add layers using **add** method or we can add all layers at init stage. The layers added are as follows:\n\n* **Conv2D** is a 2D Convolutional layer (i.e. spatial convolution over images). The parameters used are:\n * filters - the number of filters (Kernels) used with this layer; here filters = 32;\n * kernel_size - the dimmension of the Kernel: (3 x 3);\n * activation - is the activation function used, in this case `relu`;\n * kernel_initializer - the function used for initializing the kernel;\n * input_shape - is the shape of the image presented to the CNN: in our case is 28 x 28\n The input and output of the **Conv2D** is a 4D tensor.\n \n* **MaxPooling2D** is a Max pooling operation for spatial data. Parameters used here are:\n * *pool_size*, in this case (2,2), representing the factors by which to downscale in both directions;\n \n* **Dropout**. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The parameter used is:\n * *rate*, set here to 0.25. \n \n* **Conv2D** with the following parameters:\n * filters: 64;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n \n* **MaxPooling2D** with parameter:\n * *pool_size* : (2,2);\n\n* **Dropout**. with parameter:\n * *rate* : 0.25;\n \n* **Conv2D** with the following parameters:\n * filters: 128;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n\n* **Dropout**. with parameter:\n * *rate* : 0.4;\n \n* **Flatten**. This layer Flattens the input. Does not affect the batch size. It is used without parameters;\n\n* **Dense**. This layer is a regular fully-connected NN layer. It is used without parameters;\n * units - this is a positive integer, with the meaning: dimensionality of the output space; in this case is: 128;\n * activation - activation function : `relu`;\n\n* **Dropout**. with parameter:\n * *rate* : 0.3;\n \n* **Dense**. This is the final layer (fully connected). It is used with the parameters:\n * units: the number of classes (in our case 10);\n * activation : `softmax`; for this final layer it is used `softmax` activation (standard for multiclass classification)\n \n\nThen we compile the model, specifying as well the following parameters:\n* *loss*;\n* *optimizer*;\n* *metrics*. \n"
    },
    {
      "metadata": {
        "_uuid": "e31836cd5ec9d86340485404b8f613d1f574aca4",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\nmodel.add(MaxPooling2D((2, 2)))\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, \n                 kernel_size=(3, 3), \n                 activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.3))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecb450d70539a62fb310bb7ed44849d2d01481ee"
      },
      "cell_type": "markdown",
      "source": "### Inspect the model\n\nLet's check the model we initialized."
    },
    {
      "metadata": {
        "_uuid": "b4b923b11ceaf4a97677f8e24265e3e97ae1653b",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "68a2e932241f29e52a3150b3fcf3fe9c21243be2"
      },
      "cell_type": "markdown",
      "source": "### Run the model\n\nWe run the model with the training set. We are also using the validation set (a subset from the orginal training set) for validation."
    },
    {
      "metadata": {
        "_uuid": "400494b7e0525069175625422e8c300bd7b41c51",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "history = model.fit(X_train, y_train,\n          batch_size=BATCH_SIZE,\n          epochs=NO_EPOCHS,\n          verbose=1,\n          validation_data=(X_val, y_val))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8699df44bc0a95f1a43a201d3dd566746d87173f"
      },
      "cell_type": "markdown",
      "source": "\n# <a id=\"6\">Prediction accuracy</a>\n\n\n## <a id=\"61\">Evaluate the model score</a>\n\nWe calculate the test loss and accuracy."
    },
    {
      "metadata": {
        "_uuid": "b9e2bb7f25b02d491e34dd0d6d05943e287ae369",
        "trusted": true
      },
      "cell_type": "code",
      "source": "score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4d779b0e47b8263370031c292a231b69decad374"
      },
      "cell_type": "markdown",
      "source": "Test accuracy is  almost 0.93.\n\n\nWe evaluate the model accuracy based on the predicted values for the test set."
    },
    {
      "metadata": {
        "_uuid": "e519f82cc29b0dd8c5145d612da3eb496e2b321d",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#get the predictions for the test data\npredicted_classes = model.predict_classes(X_test)\n#get the indices to be plotted\ny_true = test_data.iloc[:, 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5989dd811570fe73b1f6b0715b06c428e578669",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "p = predicted_classes[:10000]\ny = y_true[:10000]\ncorrect = np.nonzero(p==y)[0]\nincorrect = np.nonzero(p!=y)[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46acb309f135a4082b4e1cd9b98a613b888f7c7e"
      },
      "cell_type": "code",
      "source": "print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc3bfd05450aaee55c9e5edc7fb84d884c1d3e0b"
      },
      "cell_type": "code",
      "source": "target_names = [\"Class {} ({}) :\".format(i,labels[i]) for i in range(NUM_CLASSES)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4b0b88588bd3afed148a3c696d997aea4a6c564a"
      },
      "cell_type": "markdown",
      "source": "\nThe best accuracy is obtained for Class 1, Class 5, Class 8, Class 9  and Class 7. Worst accuracy is for Class 6.   \n\nThe recall is highest for Class 8, Class 5 and smallest for Class 6 and Class 4.    \n\nf1-score is highest for Class 1, Class 5 and Class 8 and smallest for Class 6 followed by Class 4 and Class 2.  \n\nLet's also inspect some of the images. We created two subsets of the predicted images set, correctly and incorrectly classified."
    },
    {
      "metadata": {
        "_uuid": "614afbc7ed760856cf9b47f879334902d1b6545b"
      },
      "cell_type": "markdown",
      "source": "\n## <a id=\"62\">Correctly classified images</a>\n\n\nWe visualize few images correctly classified."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e64edf6e7ae3146eafe8da01f4dbc4e05c8bc1ce"
      },
      "cell_type": "code",
      "source": "def plot_images(data_index,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(4,4, figsize=(15,15))\n\n    for i, indx in enumerate(data_index[:16]):\n        ax[i//4, i%4].imshow(X_test[indx].reshape(IMG_ROWS,IMG_COLS), cmap=cmap)\n        ax[i//4, i%4].axis('off')\n        ax[i//4, i%4].set_title(\"True:{}  Pred:{}\".format(labels[y_true[indx]],labels[predicted_classes[indx]]))\n    plt.show()    \n    \nplot_images(correct, \"Greens\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c7bb1b300a4530d2fa418eb56258a54587e3b90"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"63\">Incorrectly classified images</a>\n\nLet's see also few images incorrectly classified."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18b2843f6f8542cd95d0847617e84e959996341b"
      },
      "cell_type": "code",
      "source": "plot_images(incorrect, \"Reds\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e24053db4dbfec25fd1b90391586d7ce6bc32988"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"7\">Conclusions</a>\n\nWith a complex sequential model with multiple convolution layers and 50 epochs for the training, we obtained an accuracy ~0.93. Only few classes are not correctly classified all the time, especially Class 6 (Shirt) and Class 2 (Pullover)."
    },
    {
      "metadata": {
        "_uuid": "1b45e3422fb9507da66ce7af67a97b54e7fa7683"
      },
      "cell_type": "markdown",
      "source": "# <a id=\"8\">References</a>\n\n[1] Fashion MNIST, An MNIST-like dataset of 70,000 28x28 labeled fashion images, https://www.kaggle.com/zalando-research/fashionmnist  \n[2] DanB, CollinMoris, Deep Learning From Scratch, https://www.kaggle.com/dansbecker/deep-learning-from-scratch  \n[3] DanB, Dropout and Strides for Larger Models, https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models  \n[4] BGO, CNN with Keras, https://www.kaggle.com/bugraokcu/cnn-with-keras    \n[5] NAIN, EagerFMINST, https://www.kaggle.com/aakashnain/eagerfmnist\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}