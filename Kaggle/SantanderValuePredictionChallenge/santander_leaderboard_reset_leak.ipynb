{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n",
      "2.0.10\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul 19 18:57:00 2018\n",
    "@from: https://www.kaggle.com/tezdhar/breaking-lb-fresh-start\n",
    "@author: gpreda\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "\n",
    "IS_LOCAL = True\n",
    "\n",
    "import os\n",
    "\n",
    "if(IS_LOCAL):\n",
    "    PATH=\"../input/santander-value-prediction-challenge/\"\n",
    "else:\n",
    "    PATH=\"../input/\"\n",
    "\n",
    "print(os.listdir(PATH))    \n",
    "print(lgb.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH+\"train.csv\")\n",
    "test = pd.read_csv(PATH+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values\n",
    "\n",
    "#Updated: Based on Jiazhen's kernel. It makes sense to have all 40 columns which seem to have differentn properties than other columns\n",
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "        '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "        'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b',\n",
    "        '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992',\n",
    "        'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd',\n",
    "        '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2',\n",
    "        '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "CPU_CORES = 1\n",
    "def _get_leak(df, cols, lag=0):\n",
    "    \"\"\" To get leak value, we do following:\n",
    "       1. Get string of all values after removing first two time steps\n",
    "       2. For all rows we shift the row by two steps and again make a string\n",
    "       3. Just find rows where string from 2 matches string from 1\n",
    "       4. Get 1st time step of row in 3 (Currently, there is additional condition to only fetch value if we got exactly one match in step 3)\"\"\"\n",
    "    series_str = df[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "    series_shifted_str = df[cols].shift(lag+2, axis=1)[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "    target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str)[0])\n",
    "    target_vals = target_rows.apply(lambda x: df.loc[x[0], cols[lag]] if len(x)==1 else 0)\n",
    "    return target_vals\n",
    "\n",
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df =  df.copy()\n",
    "    #with Pool(processes=CPU_CORES) as p:\n",
    "    #     res = [p.apply_async(_get_leak, args=(df, cols, i)) for i in range(nlags)]\n",
    "    #     res = [r.get() for r in res]\n",
    "\n",
    "    for i in range(nlags):\n",
    "        print(\"Processing lag {}\".format(i))\n",
    "        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53801/53801 [07:24<00:00, 121.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53801/53801 [07:21<00:00, 121.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53801/53801 [06:39<00:00, 134.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53801/53801 [07:03<00:00, 127.09it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['leaked_target_0', 'leaked_target_1', 'leaked_target_2',\n       'leaked_target_3'],\n      dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1df99687ae23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mleaky_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"leaked_target_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleaky_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleaky_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   4667\u001b[0m         \u001b[1;31m# For SparseDataFrame's benefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4668\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[1;32m-> 4669\u001b[1;33m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[0;32m   4670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4671\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   4682\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[0;32m   4683\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4684\u001b[1;33m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[0;32m   4685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4686\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[1;32m--> 575\u001b[1;33m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[1;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[0;32m   4699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4700\u001b[0m             raise ValueError('columns overlap but no suffix specified: %s' %\n\u001b[1;32m-> 4701\u001b[1;33m                              to_rename)\n\u001b[0m\u001b[0;32m   4702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4703\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['leaked_target_0', 'leaked_target_1', 'leaked_target_2',\n       'leaked_target_3'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "test[\"target\"] = train[\"target\"].mean()\n",
    "\n",
    "all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "all_df.head()\n",
    "\n",
    "\n",
    "NLAGS = 4 #Increasing this might help push score a bit\n",
    "all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)\n",
    "\n",
    "leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\n",
    "train = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\n",
    "test = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\n",
    "\n",
    "train[[\"target\"]+leaky_cols].head(10)\n",
    "\n",
    "train[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "test[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak values found in train and test  3598 7770\n",
      "% of correct leaks values in train  0.976097832129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79392640574417683"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We start with 1st lag target and recusrsively fill zero's\n",
    "train[\"compiled_leak\"] = 0\n",
    "test[\"compiled_leak\"] = 0\n",
    "for i in range(NLAGS):\n",
    "    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    \n",
    "print(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\n",
    "print(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])/sum(train[\"compiled_leak\"] > 0))\n",
    "\n",
    "train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nonzero_mean\"]\n",
    "test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nonzero_mean\"]\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program_Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#submission\n",
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test[\"compiled_leak\"]\n",
    "sub.to_csv(\"baseline_submission_with_leaks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
